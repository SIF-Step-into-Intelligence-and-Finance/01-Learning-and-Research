# 朴素贝叶斯

**用客观的新信息更新我们最初关于某个事物的信念后，我们就会得到一个新的、改进了的信念。——$Thomas Bayes$**

***

### 频率派

**频率学派相信概率是一个确定的值，讨论概率的分布是没有意义的。虽然没有上帝视角，无法知道具体的概率值，但我们相信概率就是确定的，它就在那里。而数据是由这个确定的概率产生的，因此数据是随机的。**

* 机器学习中的频率派

  假设我们讨论有监督学习的参数模型，那么整个过程就是用训练数据拟合出**一组参数**来，即形成一个模型，然后再预测未来的数据。

  机器学习中频率统计的应用也是一样的，只不过不求概率了，而是求参数。这就引出了另外一个概念**似然函数**。似然和概率意思差不多，区别如下：

  对于一个函数：$P(\boldsymbol x|\boldsymbol \theta)$。

  输入有两个：$\boldsymbol x$表示某一个具体的数据；$\boldsymbol \theta$表示模型的参数。

  >  如果$\boldsymbol \theta$是已知确定的，这个函数叫做**概率函数**$(probability\ function)$，它描述对于**不同的**样本点$\boldsymbol x$，其出现概率是多少。
  >  如果$\boldsymbol x$是已知确定的，这个函数叫做**似然函数**$(likelihood\ function)$，它描述对于**不同的**模型参数$\boldsymbol \theta$，出现$\boldsymbol x$这个样本点的概率是多少。

  在机器学习中，频率学派的思想没有变化，只是调换了一下位置，改为求参数（相信参数是确定的）。

  由此，又可以展开**最大似然估计**，频率统计中最常使用的最优化方法，即让似然概率最大化，也就是**固定参数**的前提下，数据出现的条件概率最大化。比如，在逻辑回归参数模型中使用。

### 贝叶斯派

**贝叶斯学派认为待估计值的概率是随机变量，而数据反过来是确定的，讨论观测数据的概率分布才是没有意义的。**

我们并没有什么上帝视角，怎么会知道最后求得的参数就是实际的真实值呢？另外，如果观测的事件不是随机的变量，而是确定的，那么频率学派对概率的解读就是不成立的。
$$
P(E|H)=\dfrac{P(E)\cdot P(H|E)}{P(H)}=P(E)\cdot\dfrac{P(H|E)}{P(E)}
$$


### 贝叶斯公式

$$
P(H|E)=\dfrac{P(H)\cdot P(E|H)}{P(E)}=P(H)\cdot\dfrac{P(E|H)}{P(E)}
$$

* $What\ is\ it\ saying?$
  $$
  \large后验=先验\times \dfrac{似然}{边缘似然}
  $$
  $E$是观测到的证据。

  $H$是假设的事件。

  $P(H)$表示先验概率。顾名思义，即事件**先于本次实验**发生的概率，通常是我们在观测到证据之前就可以确定（假定）的概率。

  $P(E|H)$称为**似然概率**$(likelihood)$，即在假设成立时$E$证据出现的概率（因为可能有其他证据导致$H$出现）。

  $P(E)$可以看成是证据出现的概率。

  $P(H|E)$表示后验概率。顾名思义，即事件在**本次实验之后**出现的概率。

* $Why\ is\ it\ true?$
  $$
  P(EH)=P(E)P(H|E)=P(H)P(E|H)=P(HE)
  $$

* $When\ is\ it\ useful?$

  在给定的假设事件成立的条件下看到证据的概率比反过来更容易考虑。

## 朴素贝叶斯法的学习与分类

朴素贝叶斯法由两部分组成，**“朴素”**是一种带有假设的限定条件，**“贝叶斯”**指的是贝叶斯公式。合起来，朴素贝叶斯指的就是在“朴素”假设条件下运用“贝叶斯公式”。       

***

### 基本方法

朴素贝叶斯法通过训练数据集**学习联合概率分布$P(X,Y)$**。具体地，学习以下**先验概率分布**及**条件概率分布**，进而利用**乘法公式**得到联合概率分布：（为什么不直接估计$P(X,Y)$呢？接下来会说。）

* **先验概率分布：**$P(Y=c_k),k=1,2\dots,K$
* **条件概率分布：**$P(\boldsymbol X=\boldsymbol x|Y=c_k)=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)},\dots,\boldsymbol X^{(n)}=\boldsymbol x^{(n)}|Y=c_k),k=1,2\dots,K$
* **联合概率分布：**$P(\boldsymbol X=\boldsymbol x,Y=c_k)=P(Y=c_k)\cdot P(\boldsymbol X=\boldsymbol x|Y=c_k),k=1,2\dots,K$

其中，我们将条件概率分布变形成如下形式：
$$
\begin{aligned}
P(\boldsymbol X=\boldsymbol x|Y=c_k)&=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)},\dots,\boldsymbol X^{(n)}=\boldsymbol x^{(n)}|Y=c_k)\\
&=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)}|Y=c_k)\cdot P(\boldsymbol X^{(2)}=\boldsymbol x^{(2)}|\boldsymbol X^{(1)}=\boldsymbol x^{(1)},Y=c_k)\cdots  P(\boldsymbol X^{(n)}=\boldsymbol x^{(n)}|\boldsymbol X^{(1)}=\boldsymbol x^{(1)},\cdots \boldsymbol X^{(n-1)}=\boldsymbol x^{(n-1)},Y=c_k)
\end{aligned}
$$
由上式可以看出，这会导致两个问题：

* 条件概率分布有指数级数量的参数，使得计算不可行。
* 在现实应用中，特征空间的大小往往远大于训练样本数，也就是说，很多样本值在训练集中根本没有出现，直接使用频率来估计是不可行的（会得到$0$），**因为未被观测到与出现概率为$0$通常是不一样的。**

其实我们不用条件概率公式直接估计的话也会出现上述两个问题：
$$
P(X=x,Y=c_k)=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)},\dots,\boldsymbol X^{(n)}=\boldsymbol x^{(n)},Y=c_k)
$$
之所以用条件概率公式，是因为下面我们**要假设特征之间是相互独立的**，但是特征与类之间是肯定不独立的，所以我们要将两者分开写。

此时我们引入**条件独立性的假设**：用于分类的特征在类确定的条件下都是条件独立的。我们就可以将公式变为下面的形式：
$$
\begin{aligned}
P(\boldsymbol X=\boldsymbol x|Y=c_k)&=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)},\dots,\boldsymbol X^{(n)}=\boldsymbol x^{(n)}|Y=c_k)\\
&=P(\boldsymbol X^{(1)}=\boldsymbol x^{(1)}|Y=c_k)\times P(\boldsymbol X^{(2)}=\boldsymbol x^{(2)}|Y=c_k)\times\cdots\times P(\boldsymbol X^{(n)}=\boldsymbol x^{(n)}|Y=c_k)
\end{aligned}
$$
可以发现上述两个问题都能很好的解决：参数明显变少，并且得到$0$的概率大大减少。

***

朴素贝叶斯将**后验概率最大的类**作为$\boldsymbol x$的类输出。后验概率的计算通过贝叶斯定理进行：

* **后验概率：**

$$
P(Y=c_k|X=\boldsymbol x)=\dfrac{P(\boldsymbol X=\boldsymbol x|Y=c_k)P(Y=c_k)}{{\Large\sum}\limits _k P(\boldsymbol X=\boldsymbol x|Y=c_k)P(Y=c_k)}
$$

引入条件独立性假设后得到：
$$
P(Y=c_k|X=\boldsymbol x)=\dfrac{P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}{{\Large\sum}\limits _k P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}
$$
以上就是朴素贝叶斯分类的基本公式。于是，朴素贝叶斯分类器表示为：
$$
y=f(x)=arg\ \mathop{max}\limits_{c_k}\dfrac{P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}{{\Large\sum}\limits _k P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}
$$
注意到，上式中分母对于所有的$c_k$都是相同的，所以可化简为如下形式：
$$
y=f(x)=arg\ \mathop{max}\limits_{c_k}\ {P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}
$$

### 后验概率最大化

#### 损失函数和风险函数

损失函数（代价函数）度量模型**一次预测**的好坏，风险函数度量**平均意义下**模型预测的好坏。

用一个损失函数（非负实值函数）来度量预测的错误程度，记作$L(Y,f(\boldsymbol X))$。常用的有$0-1$损失函数，平方损失函数，绝对损失函数，对数损失函数。

损失函数的期望：
$$
R_{exp}(f)=E_P(L(Y,f(\boldsymbol X)))
$$
这是理论上模型$f(\boldsymbol X)$关于联合分布$P(\boldsymbol X,\boldsymbol Y)$的平均意义下的损失，又叫做**风险函数**或**期望损失**。

学习的目标是选择**期望风险最小**的模型，但是联合概率分布$P(\boldsymbol X,\boldsymbol Y)$是未知的，无法计算，只能估计。

***

#### 经验风险（经验损失）

给定一个训练数据集
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots(x_N,y_N)\}
$$
模型$f(\boldsymbol X)$关于训练数据集的平均损失称为**经验风险$(empirical\ loss)$**，记作$R_{emp}$：
$$
R_{emp}(f)=\dfrac{1}{N}\sum\limits_{i=1}^NL(y_i,f(x_i))
$$
根据大数定律，当样本容量$N$趋于无穷时，$R_{emp}(f)$趋于$R_{exp}(f)$。所以一个很自然的想法就是用经验风险估计期望风险。

**但是，**由于现实中样本数目有限，甚至很小，不满足大数定律，估计结果有时候并不理想，所以我们要进行一定的矫正。

#### 经验风险最小化和结构风险最小化

顾名思义，经验风险最小化的策略认为，经验风险最小的模型就是最优的模型，当样本容量足够大时，即基本满足大数定律时，它能保证有很好的效果。
$$
\mathop{min}\limits_{f\in\cal{F}}\dfrac{1}{N}\sum\limits_{i=1}^{N}L(y_i,f(x_i))
$$
但是样本容量很小时，经验风险最小化学习会产生**过拟合**现象。

**结构风险最小化是为了防止过拟合而提出来的策略。**

其实就是在经验风险上加上表示模型复杂度的正则化项或惩罚项。
$$
R_{srm}(f)=\dfrac{1}{N}\sum\limits_{i=1}^NL(y_1,f(x_i))+\lambda J(f)
$$

***

贝叶斯中的后验概率最大化实质上等价于**期望风险最小化**。这里我们假设选择$0-1$损失函数。
$$
L(Y,f(\boldsymbol X))=
\left \{
\begin{aligned}
&1,Y\neq f(\boldsymbol X)\\
&0,Y=f(\boldsymbol X)
\end{aligned}
\right.
$$
则风险函数为
$$
R_{exp}(f)=E[L(Y,f(\boldsymbol X))]=E_X\sum\limits^K_{k=1}[L(c_k,f(\boldsymbol X))]P(c_k|\boldsymbol X)
$$


## 朴素贝叶斯法的参数估计

### 极大似然估计

在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$。我们可以用极大似然估计法估计相应的概率。

**先验概率**$P(Y=c_k)$的极大似然估计：
$$
P(Y=c_k)=\dfrac{\sum\limits_{i=1}^NI(y_i=c_k)}{N},k=1,2,\cdots,K
$$
**条件概率**$P(\boldsymbol X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计：
$$
P(\boldsymbol X^{(j)}=a_{jl}|Y=c_k)=\dfrac{\sum\limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum\limits_{i=1}^NI(y_i=c_k)}
$$

**值得注意的是，**我们采用了一个结论（来求解极大似然估计的参数，这是一个带参数的最优化问题），所以导致极大似然估计最后化成的形式就是令"出现的频率"作为概率。

### 学习与分类算法

* 计算先验概率及条件概率（另一个理解角度就是**大数定律**）

* 对于给定的实例，计算
  $$
  {P(Y=c_k)\mathop{\Large\Pi}\limits_{j=1}^n P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}
  $$
  
* 确定实例$\boldsymbol x$的类
  $$
  y=f(x)=arg\ \mathop{max}\limits_{c_k}\ {P(Y=c_k)\mathop{\Large\Pi}\limits_j P(\boldsymbol X^{(j)}=\boldsymbol x^{(j)}|Y=c_k)}
  $$
  **例子：**

  试由下表的训练数据学习一个朴素贝叶斯分类器并确定$\boldsymbol x=(2,S)^T$的类标记$y$。其中，$X^{(1)},X^{(2)}$为特征，取值集合分别为${1,2,3}$，$(S,M,L)$，$Y$为类标记，取值范围为${1,-1}$。

  |           | $1$  | $2$  | $3$  | $4$  | $5$  | $6$  | $7$  | $8$  | $9$  | $10$ | $11$ | $12$ | $13$ | $14$ | $15$ |
  | --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
  | $X^{(1)}$ | $1$  | $1$  | $1$  | $1$  | $1$  | $2$  | $2$  | $2$  | $2$  | $2$  | $3$  | $3$  | $3$  | $3$  | $3$  |
  | $X^{(2)}$ | $S$  | $M$  | $M$  | $S$  | $S$  | $S$  | $M$  | $M$  | $L$  | $L$  | $L$  | $M$  | $M$  | $L$  | $L$  |
  | $Y$       | $-1$ | $-1$ | $1$  | $1$  | $-1$ | $-1$ | $-1$ | $1$  | $1$  | $1$  | $1$  | $1$  | $1$  | $1$  | $-1$ |

  根据上述算法，由上表容易计算下列概率：

  * **先验概率：**
    $$
    P(Y=1)=\dfrac{9}{15},P(Y=-1)=\dfrac{6}{15}
    $$

  * **条件概率：**

  $$
  P(X^{(1)}=1|Y=1)=\dfrac{2}{9},P(X^{(1)}=2|Y=1)=\dfrac{3}{9},P(X^{(1)}=3|Y=1)=\dfrac{4}{9}\\
  P(X^{(2)}=S|Y=1)=\dfrac{1}{9},P(X^{(2)}=M|Y=1)=\dfrac{4}{9},P(X^{(2)}=L|Y=1)=\dfrac{4}{9}\\
  P(X^{(1)}=1|Y=-1)=\dfrac{3}{6},P(X^{(1)}=2|Y=-1)=\dfrac{2}{6},P(X^{(1)}=3|Y=-1)=\dfrac{1}{6}\\
  P(X^{(2)}=S|Y=-1)=\dfrac{3}{6},P(X^{(2)}=M|Y=-1)=\dfrac{2}{6},P(X^{(2)}=L|Y=-1)=\dfrac{1}{6}\\
  $$

  * **后验概率**
  
  对于给定的实例$\boldsymbol x=(2,S)^T$，计算：
  $$
  P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1)=\dfrac{9}{15}\cdot\dfrac{3}{9}\cdot\dfrac{1}{9}=\dfrac{1}{45}\\
  P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)=\dfrac{6}{15}\cdot\dfrac{2}{6}\cdot\dfrac{3}{6}=\dfrac{1}{15}
  $$
  由此可以看出$y=-1$。

### 贝叶斯估计

用极大似然估计简单又方便，但是由于样本不足，可能会出现所要估计的概率值为$0$的情况，这时会让后验概率为$0$，显然这是由于样本不足所导致的后果。解决这一问题的方法是采用贝叶斯估计：

* **先验概率**$P(Y=c_k)$的贝叶斯估计：
  $$
  P_\lambda(Y=c_k)=\dfrac{\sum\limits_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda},k=1,2,\cdots,K
  $$

* **条件概率**$P_\lambda(\boldsymbol X^{(j)}=a_{jl}|Y=c_k)$的贝叶斯估计：

$$
P_\lambda(\boldsymbol X^{(j)}=a_{jl}|Y=c_k)=\dfrac{\sum\limits_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_i=c_k)+S_j\lambda}
$$

不难看出，令$\lambda$为$0$就是极大似然估计。我们常取$\lambda=1$，这时称为拉普拉斯平滑。

***

我们让上述的例子按照拉普拉斯平滑估计概率，同样可得到$y=-1$

## 高斯朴素贝叶斯

高斯朴素贝叶斯模型是假设条件概率$P(X=x|Y=c_k)$是多元高斯分布，由之前的特征条件独立性假设，我们可以通过对每个特征的条件概率建模（也服从高斯分布），然后简单地相乘得到此多元高斯分布模型。

在$c_k$类下第$i$个词对应的高斯分布为：
$$
g(x_i|\mu_{i,c_k},\sigma_{i,c})=\dfrac{1}{\sigma_{i,c_k}\sqrt{2\pi}}{\Large e}^{-\dfrac{(x_i-\mu_{i,c_k})^2}{2\sigma_{i,c_k}^2}}
$$
其中，$\mu_{i,c_k},\sigma_{i,c_k}$表示$c_k$类下第$i$个特征的均值和方差。

由于特征之间的独立性假设，我们可以得到条件概率：
$$
P(X=x|Y=c_k)=\prod_{i=1}^d g(x_i|\mu_{i,c_k},\sigma_{i,c_k})
$$

